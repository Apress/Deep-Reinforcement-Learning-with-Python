{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MC Control \n",
    "\n",
    "We will be using MC Control on sample 4x4 grid world \n",
    "\n",
    "![GridWorld](./images/gridworld.png \"Grid World\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monte Carlo Prediction for Estimation (\"first-visit\")\n",
    "\n",
    "Monte Carlo Prediction is carried out by sampling the trajectories over many episodes and using the rewards seen in samples as estimate for state values. The backup digram is given below. Pseudo code for the algorithm is given in Fig 4-1\n",
    "\n",
    "![MC backup](./images/mc_backup.png \"MC Backup\")\n",
    "\n",
    "### Monte Carlo greedy Policy Improvement\n",
    "\n",
    "We will use $ \\epsilon$-greedy policy improvement approach as discussed in the text. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running in Colab/Kaggle\n",
    "\n",
    "If you are running this on Colab, please uncomment below cells and run this to install required dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Uncomment and execute this cell to install all the the dependencies if running in Google Colab or Kaggle\n",
    "\n",
    "## Uncomment and run for Colab\n",
    "# !pip install gymnasium==0.29.1 pygame==2.3.0 -q\n",
    "# !git clone https://github.com/nsanghi/drl-2ed\n",
    "# %cd /content/drl-2ed \n",
    "# %cd chapter4\n",
    "\n",
    "\n",
    "## Uncomment and run for Kaggle\n",
    "# !pip install gymnasium==0.29.1 pygame==2.3.0 -q\n",
    "# !git clone https://github.com/nsanghi/drl-2ed\n",
    "# %cd /content/drl-2ed \n",
    "# %cd chapter4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial imports and environment setup\n",
    "import numpy as np\n",
    "import sys\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.set()\n",
    "\n",
    "# create grid world environment\n",
    "from gridworld import GridWorldEnv\n",
    "env = GridWorldEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MC Policy Evaluation\n",
    "\n",
    "def mc_policy_eval(policy, env, discount_factor=1.0, episode_count=100):\n",
    "    \"\"\"\n",
    "    Evaluate a policy given an environment.\n",
    "\n",
    "    Args:\n",
    "        policy: [S, A]shaped matrix representing the policy. Random in our case\n",
    "        env: GridWorld env. In model free setup you have no access to\n",
    "             transition dynamics of the environment.\n",
    "             use step(a) to take an action and receive a tuple of (\n",
    "             s', r, done, terminated, info)\n",
    "             env.nS is number of states in the environment.\n",
    "             env.nA is number of actions in the environment.\n",
    "        episode_count: Number of episodes:\n",
    "        discount_factor: Gamma discount factor.\n",
    "\n",
    "    Returns:\n",
    "        Vector of length env.nS representing the value function.\n",
    "    \"\"\"\n",
    "    # Start with (all 0) state value array and a visit count of zero\n",
    "    V = np.zeros(env.nS)\n",
    "    N = np.zeros(env.nS)\n",
    "    i = 0\n",
    "\n",
    "    # run multiple episodes\n",
    "    while i < episode_count:\n",
    "\n",
    "        # collect samples for one episode\n",
    "        episode_states = []\n",
    "        episode_returns = []\n",
    "        state, _ = env.reset()\n",
    "        episode_states.append(state)\n",
    "        while True:\n",
    "            action = np.random.choice(env.nA, p=policy[state])\n",
    "            state, reward, done, _, _ = env.step(action)\n",
    "            episode_returns.append(reward)\n",
    "            if not done:\n",
    "                episode_states.append(state)\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        # update state values\n",
    "        G = 0\n",
    "        count = len(episode_states)\n",
    "        for t in range(count-1, -1, -1):\n",
    "            s, r = episode_states[t], episode_returns[t]\n",
    "            G = discount_factor * G + r\n",
    "            if s not in episode_states[:t]:\n",
    "                N[s] += 1\n",
    "                V[s] = V[s] + 1/N[s] * (G-V[s])\n",
    "\n",
    "        i = i+1\n",
    "\n",
    "    return np.array(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Greedy in the Limit with Infinite Exploration (GLIE)\n",
    "\n",
    "def GLIE(env, discount_factor=1.0, episode_count=100):\n",
    "    \"\"\"\n",
    "    Find optimal policy given an environment.\n",
    "\n",
    "    Args:\n",
    "        policy: [S, A]shaped matrix representing the policy. Random in our case\n",
    "        env: GridWorld env. In model free setup you have no access to\n",
    "             transition dynamics of the environment.\n",
    "             use step(a) to take an action and receive a tuple of (\n",
    "             s', r, done, terminated, info)\n",
    "             env.nS is number of states in the environment.\n",
    "             env.nA is number of actions in the environment.\n",
    "        episode_count: Number of episodes:\n",
    "        discount_factor: Gamma discount factor.\n",
    "\n",
    "    Returns:\n",
    "        Vector of length env.nS representing the value function.\n",
    "        policy:[S, A] shaped matrix representing the policy. Random in our case\n",
    "\n",
    "    \"\"\"\n",
    "    # Start with (all 0) state value array and state-action matrix.\n",
    "    # also initialize visit count to zero for the state-action visit count.\n",
    "    V = np.zeros(env.nS)\n",
    "    N = np.zeros((env.nS, env.nA))\n",
    "    Q = np.zeros((env.nS, env.nA))\n",
    "    # random policy\n",
    "    policy = [np.random.randint(env.nA) for _ in range(env.nS)]\n",
    "    k = 1\n",
    "    eps = 1\n",
    "\n",
    "    def argmax_a(arr):\n",
    "        \"\"\"\n",
    "        Return idx of max element in an array.\n",
    "        Break ties uniformly.\n",
    "        \"\"\"\n",
    "        max_idx = []\n",
    "        max_val = float('-inf')\n",
    "        for idx, elem in enumerate(arr):\n",
    "            if elem == max_val:\n",
    "                max_idx.append(idx)\n",
    "            elif elem > max_val:\n",
    "                max_idx = [idx]\n",
    "                max_val = elem\n",
    "        return np.random.choice(max_idx)\n",
    "\n",
    "    def get_action(state):\n",
    "        if np.random.random() < eps:\n",
    "            return np.random.choice(env.nA)\n",
    "        else:\n",
    "            return argmax_a(Q[state])\n",
    "\n",
    "    # run multiple episodes\n",
    "    while k <= episode_count:\n",
    "\n",
    "        # collect samples for one episode\n",
    "        episode_states = []\n",
    "        episode_actions = []\n",
    "        episode_returns = []\n",
    "        state,_ = env.reset()\n",
    "        episode_states.append(state)\n",
    "        while True:\n",
    "            action = get_action(state)\n",
    "            episode_actions.append(action)\n",
    "            state, reward, done, _, _ = env.step(action)\n",
    "            episode_returns.append(reward)\n",
    "            if not done:\n",
    "                episode_states.append(state)\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        # update state-action values\n",
    "        G = 0\n",
    "        count = len(episode_states)\n",
    "        for t in range(count-1, -1, -1):\n",
    "            s, a, r = episode_states[t], episode_actions[t], episode_returns[t]\n",
    "            G = discount_factor * G + r\n",
    "            N[s, a] += 1\n",
    "            Q[s, a] = Q[s, a] + 1/N[s, a] * (G-Q[s, a])\n",
    "\n",
    "        # Update policy and optimal value\n",
    "        k = k+1\n",
    "        eps = 1/k\n",
    "        # uncomment \"if\" to have higher exploration initially and\n",
    "        # then let epsilon decay after 5000 episodes\n",
    "        if k <=5000:\n",
    "           eps = 0.02\n",
    "\n",
    "        for s in range(env.nS):\n",
    "            best_action = argmax_a(Q[s])\n",
    "            policy[s] = best_action\n",
    "            V[s] = Q[s, best_action]\n",
    "\n",
    "    return np.array(V), np.array(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom print to show state values inside the grid\n",
    "def grid_print(V, k=None, shade_cell=True):\n",
    "    ax = sns.heatmap(V.reshape(env.shape),\n",
    "                     annot=True, square=True,\n",
    "                     cbar=False, cmap='Blues',\n",
    "                     xticklabels=False, yticklabels=False)\n",
    "\n",
    "    if k:\n",
    "        ax.set(title=\"K = {0}\".format(k))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy\n",
      "\n",
      " [['*' 'LEFT' 'DOWN' 'DOWN']\n",
      " ['UP' 'UP' 'RIGHT' 'DOWN']\n",
      " ['RIGHT' 'UP' 'DOWN' 'DOWN']\n",
      " ['UP' 'LEFT' 'RIGHT' '*']]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# run mc policy control GLIE\n",
    "V_pi, policy = GLIE(env, discount_factor=1.0, episode_count=50000)\n",
    "\n",
    "action_labels = {0:\"UP\", 1:\"RIGHT\", 2: \"DOWN\", 3:\"LEFT\"}\n",
    "# print policy\n",
    "optimal_actions = [action_labels[policy[s]] for s in range(env.nS)]\n",
    "optimal_actions[0] = \"*\" \n",
    "optimal_actions[-1] = \"*\" \n",
    "\n",
    "print(\"policy\\n\\n\",np.array(optimal_actions).reshape(env.shape))\n",
    "\n",
    "# print state values\n",
    "grid_print(V_pi.reshape(env.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "We see that policy and state values converge fairly well for 1000 episode simulation. There are some states which have wrong value most likely due to fast decay of epsilon. We need more exploration in initial episode which is controlled by the epsilon value. We are taking it as 1/k which reduces exploration very fast. It may be better to have epsilon=0.05 or something like that for first 100-1000 episodes and then let epsilon reduce as 1/k. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
